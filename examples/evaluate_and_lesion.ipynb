{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nnadapter.torchadapter import TorchAdapter\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recursivelistims(path):\n",
    "    \"\"\"Function to load images [.jpg, .jpeg, .png] from a given path\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    if os.path.isfile(path):\n",
    "        return [path]\n",
    "    else:\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for fname in filenames:\n",
    "                if fname.lower().endswith('.jpg') or fname.lower().endswith('.jpeg') or fname.lower().endswith(\n",
    "                        '.png'):\n",
    "                    l.append(os.path.join(path, dirpath, fname))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define properties of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model alexnet from pytorch model zoo\n"
     ]
    }
   ],
   "source": [
    "# Color-channel-Mean of training data\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "# Color-channel-Standard deviation of training data\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "# Expected image input size of the neural network\n",
    "#  (Channels, Height, Width)\n",
    "inputsize = (3, 224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  from a Torch7 model file\n",
    "nn = TorchAdapter('/home/max/workspace/Hydra/data/raw/convnet/resnet-18-imagenet.t7', mean=mean, std=std, inputsize=inputsize, use_gpu=True)\n",
    "\n",
    "#  from the pytorch model zoo\n",
    "#  nn = TorchAdapter('alexnet', mean=mean, std=std, inputsize=inputsize, use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find images, preprocess and evaluate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found: 40775\n"
     ]
    }
   ],
   "source": [
    "# Find images\n",
    "imagefiles = recursivelistims('/media/max/Googolplex/Databases/MSCOCO/test2014/')\n",
    "print('Number of images found: %s' % len(imagefiles))\n",
    "# Lets limited that to 1000 images\n",
    "imagefiles = imagefiles[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor shape: (1000, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess\n",
    "#  NNAdapter takes care of loading and normalization and returns a 4d-numpy array\n",
    "images = nn.preprocess(imagefiles)\n",
    "print('Image tensor shape: %s'%str(images.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternatively for evaluating very large sets of images:\n",
    "#  Loading e.g. 40775 images at once takes way too long and consumes too much memory\n",
    "#  We can load them in batched fashion\n",
    "\n",
    "# for bi in range(0, len(images), batchsize):\n",
    "#     batch = nn.preprocess(images[bi:(bi + batchsize)])\n",
    "#     nn.forward(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing layer activities/outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: Sequential\n",
      "features.0: Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "features.1: ReLU (inplace)\n",
      "features.2: MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "features.3: Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "features.4: ReLU (inplace)\n",
      "features.5: MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "features.6: Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.7: ReLU (inplace)\n",
      "features.8: Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.9: ReLU (inplace)\n",
      "features.10: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "features.11: ReLU (inplace)\n",
      "features.12: MaxPool2d (size=(3, 3), stride=(2, 2), dilation=(1, 1))\n",
      "classifier: Sequential\n",
      "classifier.0: Dropout (p = 0.5)\n",
      "classifier.1: Linear (9216 -> 4096)\n",
      "classifier.2: ReLU (inplace)\n",
      "classifier.3: Dropout (p = 0.5)\n",
      "classifier.4: Linear (4096 -> 4096)\n",
      "classifier.5: ReLU (inplace)\n",
      "classifier.6: Linear (4096 -> 1000)\n"
     ]
    }
   ],
   "source": [
    "# Let's list what layers are in the network\n",
    "layers = nn.get_layers()\n",
    "for identifier, layertype in layers.items():\n",
    "    print('%s: %s' % (identifier, layertype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features.0', 'features.3', 'features.6', 'features.8', 'features.10', 'classifier.1', 'classifier.4', 'classifier.6']\n"
     ]
    }
   ],
   "source": [
    "# We are only interested in the layer with parameters:\n",
    "filtered = [identifier for identifier, layertype in layers.items() if 'Conv' in layertype or 'Linear' in layertype]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0 output shape: (1000, 64, 55, 55)\n",
      "features.3 output shape: (1000, 192, 27, 27)\n",
      "features.6 output shape: (1000, 384, 13, 13)\n",
      "features.8 output shape: (1000, 256, 13, 13)\n",
      "features.10 output shape: (1000, 256, 13, 13)\n",
      "classifier.1 output shape: (1000, 4096)\n",
      "classifier.4 output shape: (1000, 4096)\n",
      "classifier.6 output shape: (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "output_by_layer = {}\n",
    "for layer in filtered:\n",
    "    output_by_layer[layer] = []\n",
    "\n",
    "batchsize = 480\n",
    "    \n",
    "for bi in range(0, len(images), batchsize):\n",
    "    batch = images[bi:(bi + batchsize)]\n",
    "    nn.forward(batch)\n",
    "    \n",
    "    for layer in filtered:\n",
    "        o = nn.get_layeroutput(layer)\n",
    "        output_by_layer[layer].append(o)\n",
    "    \n",
    "# Concatenate the batch-outputs\n",
    "for layer in filtered:\n",
    "    output_by_layer[layer] = np.concatenate(output_by_layer[layer])\n",
    "    print('%s output shape: %s' % (layer, str(output_by_layer[layer].shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesion layers\n",
    "NNAdapter allows you to access and manipulate the parameters of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of weight of layer features.0: (64, 3, 11, 11)\n",
      "Shape of bias of layer features.0: (64,)\n",
      "Predicted class: 537\n"
     ]
    }
   ],
   "source": [
    "# Access\n",
    "# e.g. 1st layer\n",
    "weights, bias = nn.get_layerparams(filtered[0])\n",
    "print('Shape of weight of layer %s: %s' % (filtered[0], str(weights.shape)))\n",
    "print('Shape of bias of layer %s: %s' % (filtered[0], str(bias.shape)))\n",
    "\n",
    "# Output of network for first image:\n",
    "o = nn.forward(batch[1][np.newaxis, ...])\n",
    "print('Predicted class: %d' % np.argmax(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 783\n"
     ]
    }
   ],
   "source": [
    "# Alter\n",
    "#  Set weights to zero\n",
    "weights.fill(0)\n",
    "nn.set_weights(filtered[0], weights)\n",
    "\n",
    "# Output of network for first image:\n",
    "o = nn.forward(batch[1][np.newaxis, ...])\n",
    "print('Predicted class: %d' % np.argmax(o))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
